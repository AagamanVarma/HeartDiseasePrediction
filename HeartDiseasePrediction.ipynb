Upload dataset to colab

from google.colab import files
uploaded = files.upload()


import pandas as pd

# Load dataset
df = pd.read_csv('heart.csv')

# Display first few rows
df.head()


Dataset Preprocessing and Cleaning


# Display dataset shape (rows x columns)
print("Dataset shape:", df.shape)

# Show summary statistics for each column
print("\nSummary statistics:")
print(df.describe())

# Check for missing values in each column
print("\nMissing values:")
print(df.isnull().sum())

# View column names
print("\nColumn names:")
print(df.columns.tolist())


Prepare features and target variable for model training

# Target variable: 'HeartDisease' column (0 = no, 1 = yes)
y = df['HeartDisease']

# Drop the target column from feature set
X = df.drop('HeartDisease', axis=1)

# One-Hot Encode categorical features (creates binary columns for categories)
X = pd.get_dummies(X, drop_first=True)  # drop_first avoids multicollinearity

# Check the shape after encoding
print("Final feature set shape:", X.shape)
X.head()


Train-test-split and scaling

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Split the dataset
# 80% for training, 20% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature Scaling
scaler = StandardScaler()

# Fit on training data only, then transform both
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


Model training and evaluation


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Step 5.1: Train the model
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Step 5.2: Make predictions
y_pred = model.predict(X_test_scaled)

# Step 5.3: Evaluate performance
print("Classification Report:\n")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))


train multiple models

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Step 6: Compare multiple classifiers

models = {
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "Gradient Boosting": GradientBoostingClassifier()
}

# Store results
results = {}

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"{name} Accuracy: {acc:.4f}")


Among all classifiers tested, the Support Vector Machine (SVM) model achieved the highest accuracy (87.50%), followed closely by ensemble models such as Random Forest and Gradient Boosting. These results demonstrate the effectiveness of non-linear classifiers for predicting heart disease.

Explaining model with SHAP (SHapley Additive exPlanations)

import shap

# Create an explainer (use KernelExplainer for SVM)
explainer = shap.KernelExplainer(model.predict, X_train_scaled[:100])  # Use a small background set

# Calculate SHAP values for test set
shap_values = explainer.shap_values(X_test_scaled[:50])  # Explain first 50 samples

# Plot summary plot to show feature importance globally
shap.summary_plot(shap_values, X_test.iloc[:50], feature_names=X.columns)

